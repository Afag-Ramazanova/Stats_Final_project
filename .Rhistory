# Data cleaning: filter missing values and process variables
print('here')
cleaned_data <- data %>%
mutate(
age = as.numeric(age),
urate = as.numeric(urate),
gender = as.factor(gender),
pov = as.numeric(pov),
share_white = as.numeric(share_white),
share_black = as.numeric(share_black),
share_hispanic = as.numeric(share_hispanic),
p_income = as.numeric(p_income),
raceethnicity = ifelse(raceethnicity == "Unknown", NA,raceethnicity)
) %>%
filter(!is.na(raceethnicity) & !is.na(comp_income) & !is.na(pov) & !is.na(age) & !is.na(urate) & !is.na(armed)) %>%
mutate(
armed = factor(ifelse(armed %in% c("No", "Vehicle","Disputed"), "No", "Yes")),
raceethnicity = factor(raceethnicity),
raceethnicity = relevel(raceethnicity, ref = "White")
)
# Display summary of cleaned data
glimpse(cleaned_data)
par(mar = c(4, 4, 2, 2)) # Reduce margins
par(mfrow = c(2, 1))
hist(cleaned_data$age, breaks = 20, main = "Age Distribution", xlab = "Age")
hist(cleaned_data$urate, breaks = 20, main = "Unemployment Rate Distribution", xlab = "Unemployment Rate")
race_summary <- cleaned_data %>% count(raceethnicity) %>% mutate(Percentage = round(n / sum(n) *100,2))
kable(race_summary, caption = "Race Ethinicity Summary Statistics")
rq1_data <- cleaned_data %>% filter(raceethnicity %in% c("Black", "White", "Hispanic/Latino"))
library(reshape2)
library(ggplot2)
library(viridis)
# options(repr.plot.width = 5, repr.plot.height = 1)
ggplot(rq1_data, aes(x = comp_income, y = raceethnicity, fill = raceethnicity)) +
geom_boxplot(alpha = 0.7) +
scale_fill_viridis(discrete = TRUE) +
labs(title = "Economic Conditions and Racial Composition",
x = "Relative Income",
y = "Race/Ethnicity") +
theme_minimal() +
theme(
plot.title = element_text(size = 14, face = "bold"),
axis.title = element_text(size = 12),
axis.text = element_text(size = 10),
legend.position = "none"
) +
coord_flip()
# Create correlation matrix
cor_matrix <- cor(rq1_data[, c("comp_income", "pov", "nat_bucket", "county_bucket", "p_income", "urate")], use = "complete.obs")
# Melt the correlation matrix
cor_melted <- melt(cor_matrix)
name_mapping <- c(
"comp_income" = "Relative Income",
"pov" = "Poverty Rate",
"nat_bucket" = "National Income Tier",
"county_bucket" = "County Income Tier",
"p_income" = "Personal Income Tier",
"urate" = "Unemployment Rate"
)
library(scales)
# options(repr.plot.width = 5, repr.plot.height =4)
# Create correlation plot with viridis colors
ggplot(cor_melted, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() +
scale_fill_viridis(option = "viridis", limits = c(-1, 1)) +
geom_text(aes(label = round(value, 2)),
color = ifelse(abs(cor_melted$value) > 0.5, "white", "black"),
size = 4) +
labs(title = "Correlation Plot of Economic Related Indicators",
x = "", y = "") +
scale_x_discrete(labels = \(x) label_wrap(10)(name_mapping[x])) +
scale_y_discrete(labels = \(x) label_wrap(10)(name_mapping[x])) +
theme_minimal() +
theme(
plot.title = element_text(size = 14, face = "bold"),
axis.text = element_text(size =7),
# axis.text.x = element_text(angle = 45, hjust = 1),
panel.grid.major = element_blank(),
panel.border = element_blank()
)
cols_to_numeric <- c(
"p_income", "h_income", "comp_income", "county_income",
"urate", "college", "pov")
cor_matrix <- cor(cleaned_data[cols_to_numeric])
print(cor_matrix)
cleaned_data <- cleaned_data[!(cleaned_data$raceethnicity %in% c("Asian/Pacific Islander", "Native American")), ]
cleaned_data$raceethnicity <- factor(cleaned_data$raceethnicity)
cleaned_data$raceethnicity <- relevel(cleaned_data$raceethnicity, ref = "White")
# Multinomial logistic regression
rq1_model2 <- nnet::multinom(
raceethnicity ~ comp_income + pov + urate + college ,
data = cleaned_data
)
confusionMatrix(predict(rq1_model2), cleaned_data$raceethnicity, mode="everything")
intercept_tablee <- as.data.frame(cbind(
Coefficients = summary(rq1_model2)$coefficients[, "(Intercept)"],
Std.Errors = summary(rq1_model2)$standard.errors[, "(Intercept)"],
p.values = 2 * (1 - pnorm(abs(summary(rq1_model2)$coefficients[, "(Intercept)"] /summary(rq1_model2)$standard.errors[, "(Intercept)"])))))
intercept_tablee$Variable <- "Intercept"
intercept_tablee
comp_income_tablee <- as.data.frame(cbind(
Coefficients = summary(rq1_model2)$coefficients[, "comp_income"],
Std.Errors = summary(rq1_model2)$standard.errors[, "comp_income"],
p.values = 2 * (1 - pnorm(abs(summary(rq1_model2)$coefficients[, "comp_income"] /summary(rq1_model2)$standard.errors[, "comp_income"])))))
comp_income_tablee$Variable <- "Relative Income"
pov_tablee <- as.data.frame(cbind(
Coefficients = summary(rq1_model2)$coefficients[, "pov"],
Std.Errors = summary(rq1_model2)$standard.errors[, "pov"],
p.values = 2 * (1 - pnorm(abs(summary(rq1_model2)$coefficients[, "pov"] /summary(rq1_model2)$standard.errors[, "pov"])))))
pov_tablee$Variable <- "Poverty Rate"
urate_tablee <- as.data.frame(cbind(
Coefficients = summary(rq1_model2)$coefficients[, "urate"],
Std.Errors = summary(rq1_model2)$standard.errors[, "urate"],
p.values = 2 * (1 - pnorm(abs(summary(rq1_model2)$coefficients[, "urate"] /
summary(rq1_model2)$standard.errors[, "urate"])))))
urate_tablee$Variable <- "Unemployment Rate"
college_tablee <- as.data.frame(cbind(
Coefficients = summary(rq1_model2)$coefficients[, "college"],
Std.Errors = summary(rq1_model2)$standard.errors[, "college"],
p.values = 2 * (1 - pnorm(abs(summary(rq1_model2)$coefficients[, "college"] /summary(rq1_model2)$standard.errors[, "college"])))))
college_tablee$Variable <- "Higher Education Share"
exp(confint(rq1_model2))
combined_table <- rbind(intercept_tablee, comp_income_tablee, pov_tablee, urate_tablee, college_tablee #, share_black_tablee
)
combined_table <- combined_table %>%
mutate(
Odds_Ratio = exp(Coefficients),
CI_Lower = exp(Coefficients - 1.96 * Std.Errors),
CI_Upper = exp(Coefficients + 1.96 * Std.Errors)
)
combined_table <- cbind(`Race Ethnicity` = rownames(combined_table), combined_table)
combined_table <- combined_table[, c("Variable", "Coefficients", "Std.Errors", "Odds_Ratio", "CI_Lower", "CI_Upper", "p.values")]
combined_table <- combined_table %>%
mutate(
Coefficients = round(Coefficients, 2),
Std.Errors = round(Std.Errors, 2),
Odds_Ratio = round(Odds_Ratio, 2),
CI_Lower = round(CI_Lower, 2),
CI_Upper = round(CI_Upper, 2),
p.values = round(p.values, 2)
)
combined_table$Significance <- ifelse(
combined_table$p.values < 0.001, "***",
ifelse(combined_table$p.values < 0.01, "**",
ifelse(combined_table$p.values < 0.05, "*", ""))
)
kable(combined_table, caption = "Combined Table: Coefficients, Std.Errors, Odds Ratios, Confidence Intervals, and p-values",
col.names = c("Variable", "Coef.", "SE", "OR", "CI Low", "CI Up", "p-val", "Sig."),
align = c('l', rep('r', 7)),  # Left align text, right align numbers
format = "latex",
booktabs = TRUE)
# Armed Status Distribution
armed_counts <- table(cleaned_data$armed)
armed_percentages <- prop.table(armed_counts) * 100
armed_summary <- data.frame(
Armed_Status = names(armed_counts),
Count = as.integer(armed_counts),
Percentage = round(as.numeric(armed_percentages), 2) # Round percentages for clarity
)
pander(armed_summary, caption  = "Armed Status Distribution")
# Scatter plot
ggplot(cleaned_data, aes(x = age, y = urate, color = armed, shape = armed)) +
geom_point(size = 3, alpha = 0.5) +
labs(
title = "Scatter Plot of Unemployment Rate vs. Age by Armed Status",
x = "Age",
y = "Unemployment Rate (%)",
color = "Armed Status",
shape = "Armed Status"
) +
theme_minimal(base_size = 14) +
theme(
legend.position = "top",
plot.title = element_text(hjust = 0.5)
)
# Plot 2: Relationship between 'armed' and 'age'
plot4 <- ggplot(cleaned_data, aes(x = armed, y = age)) +
geom_boxplot(fill = "lightblue", outlier.colour = "red", outlier.shape = 1) +
geom_jitter(width = 0.2, alpha = 0.2) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Relationship between Armed Status and Age",
x = "Armed Status",
y = "Age")
plot4
# Plot 2: Relationship between 'armed' and 'age'
plot5 <- ggplot(cleaned_data, aes(x = armed, y = urate)) +
geom_boxplot(fill = "lightblue", outlier.colour = "red", outlier.shape = 1) +
geom_jitter(width = 0.2, alpha = 0.2) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Relationship between Armed Status and Unemplyement Rate",
x = "Armed Status",
y = "Unemployment Rate")
plot5
# Armed Status Distribution
armed_counts <- table(cleaned_data$armed)
armed_percentages <- prop.table(armed_counts) * 100
armed_summary <- data.frame(
Armed_Status = names(armed_counts),
Count = as.integer(armed_counts),
Percentage = round(as.numeric(armed_percentages), 2) # Round percentages for clarity
)
pander(armed_summary, caption  = "Armed Status Distribution")
# Scatter plot
ggplot(cleaned_data, aes(x = age, y = urate, color = armed, shape = armed)) +
geom_point(size = 3, alpha = 0.5) +
labs(
title = "Scatter Plot of Unemployment Rate vs. Age by Armed Status",
x = "Age",
y = "Unemployment Rate (%)",
color = "Armed Status",
shape = "Armed Status"
) +
theme_minimal(base_size = 14) +
theme(
legend.position = "top",
plot.title = element_text(hjust = 0.5)
)
# Plot 2: Relationship between 'armed' and 'age'
plot4 <- ggplot(cleaned_data, aes(x = armed, y = age)) +
geom_boxplot(fill = "lightblue", outlier.colour = "red", outlier.shape = 1) +
geom_jitter(width = 0.2, alpha = 0.2) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Relationship between Armed Status and Age",
x = "Armed Status",
y = "Age")
plot4
# Plot 2: Relationship between 'armed' and 'age'
plot5 <- ggplot(cleaned_data, aes(x = armed, y = urate)) +
geom_boxplot(fill = "lightblue", outlier.colour = "red", outlier.shape = 1) +
geom_jitter(width = 0.2, alpha = 0.2) +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
labs(title = "Relationship between Armed Status and Unemplyement Rate",
x = "Armed Status",
y = "Unemployment Rate")
plot5
# Logistic regression
rq2_model <- glm(
armed ~ (age + urate +  college + pov)^2,  data = cleaned_data,
family = binomial
)
# Display the summary of the updated model
pander(summary(rq2_model))
# Check for multicollinearity using Variance Inflation Factor (VIF)
vif_values <- vif(rq2_model)
# Logistic regression
library(car)
rq2_model <- glm(
armed ~ (age + urate +  college + pov)^2,  data = cleaned_data,
family = binomial
)
# Display the summary of the updated model
pander(summary(rq2_model))
# Check for multicollinearity using Variance Inflation Factor (VIF)
vif_values <- vif(rq2_model)
print(vif_values)
# Diagnostic plots
par(mfrow = c(2, 2)) # Set plotting area to 2x2
# Residual vs Fitted
plot(rq2_model$fitted.values, residuals(rq2_model, type = "deviance"),
xlab = "Fitted Values",
ylab = "Deviance Residuals",
main = "Residuals vs Fitted",
pch = 20, col = "blue")
abline(h = 0, col = "red", lty = 2)
# Normal Q-Q Plot
qqnorm(residuals(rq2_model, type = "deviance"),
main = "Normal Q-Q Plot of Residuals",
pch = 20, col = "blue")
qqline(residuals(rq2_model, type = "deviance"), col = "red", lty = 2)
# Scale-Location Plot
plot(rq2_model$fitted.values, sqrt(abs(residuals(rq2_model, type = "deviance"))),
xlab = "Fitted Values",
ylab = "Sqrt |Deviance Residuals|",
main = "Scale-Location Plot",
pch = 20, col = "blue")
abline(h = 0, col = "red", lty = 2)
# Cook's Distance
plot(cooks.distance(rq2_model),
xlab = "Index",
ylab = "Cook's Distance",
main = "Cook's Distance",
pch = 20, col = "blue")
abline(h = 4/(nrow(cleaned_data) - length(rq2_model$coefficients)), col = "red", lty = 2)
par(mfrow = c(1, 1)) # Reset plotting area
# Confusion Matrix for Model Evaluation
rq2_model_pred <- ifelse(predict(rq2_model, type = "response") > 0.5, 1, 0)
rq2_model_pred_fac <- factor(rq2_model_pred, levels = c(0, 1), labels = c("No", "Yes"))
conf_matrix <- confusionMatrix(
table(rq2_model_pred_fac, cleaned_data$armed),
positive = "Yes",
mode = "everything"
)
pander(conf_matrix)
# Logistic regression
library(car)
rq2_model <- glm(
armed ~ (age + urate +  college + pov)^2,  data = cleaned_data,
family = binomial
)
# Display the summary of the updated model
pander(summary(rq2_model))
# Check for multicollinearity using Variance Inflation Factor (VIF)
vif_values <- vif(rq2_model)
print(vif_values)
# Diagnostic plots
par(mfrow = c(2, 2)) # Set plotting area to 2x2
# Residual vs Fitted
plot(rq2_model$fitted.values, residuals(rq2_model, type = "deviance"),
xlab = "Fitted Values",
ylab = "Deviance Residuals",
main = "Residuals vs Fitted",
pch = 20, col = "blue")
abline(h = 0, col = "red", lty = 2)
# Normal Q-Q Plot
qqnorm(residuals(rq2_model, type = "deviance"),
main = "Normal Q-Q Plot of Residuals",
pch = 20, col = "blue")
qqline(residuals(rq2_model, type = "deviance"), col = "red", lty = 2)
# Scale-Location Plot
plot(rq2_model$fitted.values, sqrt(abs(residuals(rq2_model, type = "deviance"))),
xlab = "Fitted Values",
ylab = "Sqrt |Deviance Residuals|",
main = "Scale-Location Plot",
pch = 20, col = "blue")
abline(h = 0, col = "red", lty = 2)
# Cook's Distance
plot(cooks.distance(rq2_model),
xlab = "Index",
ylab = "Cook's Distance",
main = "Cook's Distance",
pch = 20, col = "blue")
abline(h = 4/(nrow(cleaned_data) - length(rq2_model$coefficients)), col = "red", lty = 2)
par(mfrow = c(1, 1)) # Reset plotting area
# Confusion Matrix for Model Evaluation
rq2_model_pred <- ifelse(predict(rq2_model, type = "response") > 0.5, 1, 0)
rq2_model_pred_fac <- factor(rq2_model_pred, levels = c(0, 1), labels = c("No", "Yes"))
conf_matrix <- confusionMatrix(
table(rq2_model_pred_fac, cleaned_data$armed),
positive = "Yes",
mode = "everything"
)
pander(conf_matrix)
# Cook's Distance threshold
cooks_threshold <- 4 / nrow(cleaned_data)
cooks_values <- cooks.distance(rq2_model)
# Identify influential observations
influential_points <- which(cooks_values > cooks_threshold)
# Remove influential observations
cleaned_data_refit <- cleaned_data[-influential_points, ]
# Refit the logistic regression model
rq2_model_refit <- glm(
armed ~ (age + urate + college + pov)^2,
data = cleaned_data_refit,
family = binomial
)
# Summary of refitted model
pander(summary(rq2_model_refit))
# Re-check model assumptions with diagnostic plots
par(mfrow = c(2, 2))
plot(rq2_model_refit)
par(mfrow = c(1, 1))
# Check for significance
if (any(summary(rq2_model_refit)$coefficients[, 4] < 0.05)) {
print("Significant relationships were found after removing outliers.")
} else {
print("Results remain insignificant; no significant relationships observed.")
}
# Cook's Distance threshold
cooks_threshold <- 4 / nrow(cleaned_data)
cooks_values <- cooks.distance(rq2_model)
# Identify influential observations
influential_points <- which(cooks_values > cooks_threshold)
# Remove influential observations
cleaned_data_refit <- cleaned_data[-influential_points, ]
# Refit the logistic regression model
rq2_model_refit <- glm(
armed ~ (age + urate + college + pov)^2,
data = cleaned_data_refit,
family = binomial
)
# Summary of refitted model
pander(summary(rq2_model_refit))
# Re-check model assumptions with diagnostic plots
par(mfrow = c(2, 2))
plot(rq2_model_refit)
par(mfrow = c(1, 1))
# Check for significance
if (any(summary(rq2_model_refit)$coefficients[, 4] < 0.05)) {
print("Significant relationships were found after removing outliers.")
} else {
print("Results remain insignificant; no significant relationships observed.")
}
# Calculate Cook's Distance for refitted model
cooks_values_refit <- cooks.distance(rq2_model_refit)
# Plot Cook's Distance
plot(
cooks_values_refit,
pch = 20,
cex = 1,
col = "blue",
main = "Cook's Distance After Removing Outliers",
ylab = "Cook's Distance",
xlab = "Observation Index"
)
abline(h = cooks_threshold, col = "red", lty = 2) # Threshold line
text(
x = which(cooks_values_refit > cooks_threshold),
y = cooks_values_refit[cooks_values_refit > cooks_threshold],
labels = which(cooks_values_refit > cooks_threshold),
pos = 3,
cex = 0.7,
col = "red"
)
# Logistic regression
library(car)
rq2_model <- glm(
armed ~ (age + urate +  college + pov)^2,  data = cleaned_data,
family = binomial
)
# Display the summary of the updated model
pander(summary(rq2_model))
# Check for multicollinearity using Variance Inflation Factor (VIF)
vif_values <- vif(rq2_model)
print(vif_values)
# Diagnostic plots
par(mfrow = c(2, 2)) # Set plotting area to 2x2
# Residual vs Fitted
plot(rq2_model$fitted.values, residuals(rq2_model, type = "deviance"),
xlab = "Fitted Values",
ylab = "Deviance Residuals",
main = "Residuals vs Fitted",
pch = 20, col = "blue")
abline(h = 0, col = "red", lty = 2)
# Normal Q-Q Plot
qqnorm(residuals(rq2_model, type = "deviance"),
main = "Normal Q-Q Plot of Residuals",
pch = 20, col = "blue")
qqline(residuals(rq2_model, type = "deviance"), col = "red", lty = 2)
# Scale-Location Plot
plot(rq2_model$fitted.values, sqrt(abs(residuals(rq2_model, type = "deviance"))),
xlab = "Fitted Values",
ylab = "Sqrt |Deviance Residuals|",
main = "Scale-Location Plot",
pch = 20, col = "blue")
abline(h = 0, col = "red", lty = 2)
# Cook's Distance
plot(cooks.distance(rq2_model),
xlab = "Index",
ylab = "Cook's Distance",
main = "Cook's Distance",
pch = 20, col = "blue")
abline(h = 4/(nrow(cleaned_data) - length(rq2_model$coefficients)), col = "red", lty = 2)
par(mfrow = c(1, 1)) # Reset plotting area
# Confusion Matrix for Model Evaluation
rq2_model_pred <- ifelse(predict(rq2_model, type = "response") > 0.5, 1, 0)
rq2_model_pred_fac <- factor(rq2_model_pred, levels = c(0, 1), labels = c("No", "Yes"))
conf_matrix <- confusionMatrix(
table(rq2_model_pred_fac, cleaned_data$armed),
positive = "Yes",
mode = "everything"
)
pander(conf_matrix)
# Cook's Distance threshold
cooks_threshold <- 4 / nrow(cleaned_data)
cooks_values <- cooks.distance(rq2_model)
# Identify influential observations
influential_points <- which(cooks_values > cooks_threshold)
# Remove influential observations
cleaned_data_refit <- cleaned_data[-influential_points, ]
# Refit the logistic regression model
rq2_model_refit <- glm(
armed ~ (age + urate + college + pov)^2,
data = cleaned_data_refit,
family = binomial
)
# Summary of refitted model
pander(summary(rq2_model_refit))
# Re-check model assumptions with diagnostic plots
par(mfrow = c(2, 2))
plot(rq2_model_refit)
par(mfrow = c(1, 1))
# Check for significance
if (any(summary(rq2_model_refit)$coefficients[, 4] < 0.05)) {
print("Significant relationships were found after removing outliers.")
} else {
print("Results remain insignificant; no significant relationships observed.")
}
# Calculate Cook's Distance for refitted model
cooks_values_refit <- cooks.distance(rq2_model_refit)
# Plot Cook's Distance
plot(
cooks_values_refit,
pch = 20,
cex = 1,
col = "blue",
main = "Cook's Distance After Removing Outliers",
ylab = "Cook's Distance",
xlab = "Observation Index"
)
abline(h = cooks_threshold, col = "red", lty = 2) # Threshold line
text(
x = which(cooks_values_refit > cooks_threshold),
y = cooks_values_refit[cooks_values_refit > cooks_threshold],
labels = which(cooks_values_refit > cooks_threshold),
pos = 3,
cex = 0.7,
col = "red"
)
